{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS & PARAMS\n",
    "# if _user_selected_model is None:\n",
    "#     Execution_Id = \"output/{}/Job_{}_best_{}\".format(dataset_name, _id,datetime.datetime.now())\n",
    "# Pycaret Args\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Markdown as md\n",
    "from pyfiglet import Figlet\n",
    "import datetime\n",
    "f = Figlet(font='slant')\n",
    "print (f.renderText('SimpleML'))\n",
    "\n",
    "print(\"Type : Linear Regression\")\n",
    "print(\"Input : {}\".format(\"boston\"))\n",
    "print(\"Model selection : {}\".format(\"Auto select best or / intreseted mode\"))\n",
    "print(\"Date time : {}\".format(datetime.datetime.now()))\n",
    "\n",
    "# Read config\n",
    "\n",
    "import yaml\n",
    "# Setup Environment constants\n",
    "setup = dict(train_size = 0.7, sampling = True, sample_estimator = None, categorical_features = None, categorical_imputation = 'constant', \n",
    "\tordinal_features = None, high_cardinality_features = None, high_cardinality_method = 'frequency', numeric_features = None, \n",
    "\tnumeric_imputation = 'mean', date_features = None, ignore_features = None, normalize = False, \n",
    "\tnormalize_method = 'zscore', transformation = False, transformation_method = 'yeo-johnson', handle_unknown_categorical = True, \n",
    "\tunknown_categorical_method = 'least_frequent', pca = False, pca_method = 'linear', pca_components = None, \n",
    "\tignore_low_variance = False, combine_rare_levels = False, rare_level_threshold = 0.10, \n",
    "\tbin_numeric_features = None, remove_outliers = False, outliers_threshold = 0.05, remove_multicollinearity = False, \n",
    "\tmulticollinearity_threshold = 0.9, create_clusters = False, cluster_iter = 20, polynomial_features = False, polynomial_degree = 2, \n",
    "\ttrigonometry_features = False, polynomial_threshold = 0.1, group_features = None, group_names = None, feature_selection = False, \n",
    "\tfeature_selection_threshold = 0.8, feature_interaction = False, feature_ratio = False, interaction_threshold = 0.01, transform_target = False, \n",
    "\tsession_id = None, silent=False, profile = False )\n",
    "\n",
    "# with open('new_config.yml', 'w') as outfile:\n",
    "#     yaml.dump(setup, outfile, default_flow_style=False)\n",
    "\n",
    "# Read YAML file\n",
    "with open(\"auto_config.yml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "for i,j in config.items():\n",
    "    if j == 'None':\n",
    "        config[i] = None\n",
    "        \n",
    "#assert setup == config\n",
    "{'auto_select_best_model': True, 'auto_select_best_model_by': 'R2', 'bin_numeric_features': 'False', \n",
    " 'categorical_features': 'None', 'categorical_imputation': 'constant', 'cluster_iter': '20', \n",
    " 'combine_rare_levels': 'False', 'create_clusters': 'False', 'date_features': 'None', \n",
    " 'feature_interaction': 'False', 'feature_ratio': 'False', 'feature_selection': 'False', \n",
    " 'feature_selection_threshold': '0.8', 'group_features': 'None', 'group_names': 'None', \n",
    " 'handle_unknown_categorical': 'False', 'high_cardinality_features': 'None', \n",
    " 'high_cardinality_method': 'frequency', 'ignore_features': 'None', 'ignore_low_variance': 'False', \n",
    " 'input_file': 'None', 'interaction_threshold': '0.01', 'multicollinearity_threshold': '0.9', \n",
    " 'normalize': 'False', 'normalize_method': 'zscore', 'numeric_features': 'None', \n",
    " 'numeric_imputation': 'mean', 'ordinal_features': 'None', 'outliers_threshold': '0.05', \n",
    " 'pca': 'False', 'pca_method': 'linear', 'polynomial_degree': '2', 'polynomial_features': 'False', \n",
    " 'polynomial_threshold': '0.1', 'profile': 'True', 'rare_level_threshold': 'False', \n",
    " 'regression': True, 'remove_multicollinearity': 'False', 'remove_outliers': 'False', \n",
    " 'sample_estimator': 'None', 'sampling': 'False', 'session_id': 'None', 'silent': 'True', \n",
    " 'target_variable': 'medv', 'train_size': '0.7', 'transform_target': 'False', \n",
    " 'transform_target_method': 'box-tox', 'transformation': 'False', 'transformation_method': 'yeo-johnson', \n",
    " 'trigonometry_features': 'False', 'unknown_categorical_method': 'least_frequent'}\n",
    "\n",
    "\n",
    "\n",
    "print(config)\n",
    "# CCaret config\n",
    "if config['input_file'] is None:\n",
    "    _input_file = None\n",
    "    _demo_dataset = 'boston'\n",
    "else:\n",
    "    _input_file = config['input_file']\n",
    "\n",
    "_pandas_profiling = config['profile'] # _pandas_profiling = True # Default is True which gives detailed \n",
    "\n",
    "# Target processing\n",
    "_target = config['target_variable'] # _target = 'medv' # default Target cclass for Boston data\n",
    "_target_class = config['target_variable'] # _target_class = 'medv'\n",
    "\n",
    "# Silent preprocessing should be True\n",
    "_silent_preprocessing = True # silent_preproccessing = True\n",
    "\n",
    "\n",
    "_normalize = config['normalize'] # _normalize = True\n",
    "\n",
    "_transformation = config['transformation'] # _transformation = True\n",
    "\n",
    "_transforn_target = config['transform_target'] # _transform_target = True\n",
    "_combine_rare_levels = config['combine_rare_levels'] # _combine_rare_levels = True\n",
    "_rare_level_threshold = config['rare_level_threshold'] # _rare_level_threshold = 0.05\n",
    "\n",
    "_remove_multicollinearity = config['remove_multicollinearity'] # True\n",
    "_multicollinearity_threshold = config['multicollinearity_threshold'] # 0.95\n",
    "_bin_numeric_features = [] # config[bin_numeric_features] # [] # TODO NEED TO FIX VALUES FROM CONFIG\n",
    "#_silent_preproccessing = True\n",
    "\n",
    "# Temp args\n",
    "_autoselect = config['auto_select_best_model'] # True\n",
    "_kfold = 10 # config['Kfolds'] # 10\n",
    "_round = 4\n",
    "_sort_best_model_by = config['auto_select_best_model_by'] # 'R2' #['R2']\n",
    "_turbo = True\n",
    "_blacklist = None\n",
    "_user_selected_model = None\n",
    "\n",
    "# Now create model\n",
    "number_of_models = 1\n",
    "_auto_tune = True \n",
    "import os\n",
    "x = os.getcwd()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file:///Users/LMN/Documents/githubs/myCaret/simpleMl/auto_regression/your_report.html\n",
    "#md(f\"<a href='file:///Users/LMN/Documents/githubs/myCaret/simpleMl/auto_regression/your_report.html'>ds</a>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My ARGS\n",
    "\n",
    "# Pycaret Args\n",
    "# _input_file = None\n",
    "# _demo_dataset = 'boston'\n",
    "# _pandas_profiling = True # Default is True which gives detailed \n",
    "# _target = 'medv' # default Target cclass for Boston data\n",
    "# _silent_preproccessing = True\n",
    "import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# logging.debug(\"test\")\n",
    "#import the dataset from pycaret repository\n",
    "logging.info(\"HELLLP\")\n",
    "import pandas as pd\n",
    "if _input_file == None and _demo_dataset == None:\n",
    "    from pycaret.datasets import get_data\n",
    "    input_data = get_data('boston', profile = True)\n",
    "    data = input_data.sample(frac=0.9, random_state=786).reset_index(drop=True)\n",
    "    data_unseen = input_data.drop(data.index).reset_index(drop=True)\n",
    "\n",
    "    print('Data for Modeling: ' + str(data.shape))\n",
    "    print('Unseen Data For Predictions: ' + str(data_unseen.shape))\n",
    "elif _demo_dataset in ['diabetes', 'boston']:\n",
    "    from pycaret.datasets import get_data\n",
    "    input_data = get_data(_demo_dataset, profile = True)\n",
    "    data = input_data.sample(frac=0.9, random_state=786).reset_index(drop=True)\n",
    "    data_unseen = input_data.drop(data.index).reset_index(drop=True)\n",
    "\n",
    "    print('Data for Modeling: ' + str(data.shape))\n",
    "    print('Unseen Data For Predictions: ' + str(data_unseen.shape))\n",
    "else:\n",
    "    input_data = pd.read_csv(_input_file)\n",
    "    # print('No proper input was given, So running regression on Boston dataset')\n",
    "    logging.info('Succesfully read the input csv')\n",
    "    data = input_data.sample(frac=0.9, random_state=786).reset_index(drop=True)\n",
    "    data_unseen = input_data.drop(data.index).reset_index(drop=True)\n",
    "\n",
    "    print('Data for Modeling: ' + str(data.shape))\n",
    "    print('Unseen Data For Predictions: ' + str(data_unseen.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape : {}\".format(input_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setup\n",
    "\n",
    "pre-processing steps (other than those that are imperative for machine learning experiments which were performed \n",
    "automatically by PyCaret). In this example we will take it to the next level by customizing the pre-processing pipeline using setup(). Let's look at how to implement all the steps discussed in section 4 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _target_class = 'medv'\n",
    "# _normalize = True\n",
    "# _transformation = True\n",
    "# _transform_target = True\n",
    "# _combine_rare_levels = True\n",
    "# _rare_level_threshold = 0.05\n",
    "# _remove_multicollinearity = True\n",
    "# _multicollinearity_threshold = 0.95\n",
    "# _bin_numeric_features = []\n",
    "# _silent_preproccessing = True\n",
    "from pycaret.regression import *\n",
    "\n",
    "exp_reg102 = setup(data = input_data, target = 'medv', session_id=123,\n",
    "                  normalize = True, transformation = True, transform_target = True, \n",
    "                  combine_rare_levels = True, rare_level_threshold = 0.05,\n",
    "                  remove_multicollinearity = True, multicollinearity_threshold = 0.95, \n",
    "                  bin_numeric_features = [], silent = _silent_preprocessing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing all Models\n",
    "Comparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using k-fold cross validation for metric evaluation. The output prints a score grid that shows average MAE, MSE, RMSE, R2, RMSLE and MAPE accross the folds (10 by default) of all the available models in the model library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Temp args\n",
    "# _autoselect = True\n",
    "# _kfold = 10\n",
    "# _round = 4\n",
    "# _sort_best_model_by = 'R2' #['R2']\n",
    "# _turbo = True\n",
    "# _blacklist = None\n",
    "# _user_selected_model = None\n",
    "\n",
    "models_comparison = compare_models(blacklist = _blacklist, fold = _kfold,  round = _round,  sort = _sort_best_model_by, turbo = _turbo)\n",
    "models_comparison.to_excel('tmp_models.xlsx')\n",
    "models_comparison_df = pd.read_excel('tmp_models.xlsx',index = False)\n",
    "display(models_comparison)\n",
    "best_model = models_comparison_df['Model'][0]\n",
    "top_five_best_models = models_comparison_df['Model'][0:5]\n",
    "top_three_best_models = models_comparison_df['Model'][0:3]\n",
    "print(\"Best suggested model based on best value for {} is {}\".format(best_model, _sort_best_model_by))\n",
    "if _user_selected_model is not None:\n",
    "    print('Model of interest is selected {}'.format(_user_selected_model))\n",
    "\n",
    "if _autoselect == True or _user_selected_model is None:\n",
    "    model_selected = best_model\n",
    "    print(\"Selected {} based on {}\".format(best_model, _sort_best_model_by))\n",
    "else:\n",
    "    model_selected = _user_selected_model\n",
    "    print(\"Selected {} based on user choice\".format(model_selected))\n",
    "\n",
    "#print(\"model stats for {}: DF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two simple words of code (not even a line) have created over 22 models using 10 fold cross validation and evaluated the 6 most commonly used regression metrics (MAE, MSE, RMSE, R2, RMSLE and MAPE). The score grid printed above highlights the highest performing metric for comparison purposes only. The grid by default is sorted using R2 (highest to lowest) which can be changed by passing sort parameter. For example compare_models(sort = 'RMSLE') will sort the grid by RMSLE (lower to higher since lower is better). If you want to change the fold parameter from the default value of 10 to a different value then you can use the fold parameter. For example compare_models(fold = 5) will compare all models on 5 fold cross validation. Reducing the number of folds will improve the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model\n",
    "While compare_models() is a powerful function and often a starting point in any experiment, it does not return any trained models. PyCaret's recommended experiment workflow is to use compare_models() right after setup to evaluate top performing models and finalize a few candidates for continued experimentation. As such, the function that actually allows to you create a model is unimaginatively called create_model(). This function creates a model and scores it using stratified cross validation. Similar to compare_models(), the output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold.\n",
    "\n",
    "For the remaining part of this tutorial, we will work with the below models as our candidate models. The selections are for illustration purposes only and do not necessarily mean they are the top performing or ideal for this type of data.\n",
    "\n",
    " - AdaBoost Regressor ('ada')\n",
    " - Light Gradient Boosting Machine ('lightgbm')\n",
    " - Decision Tree ('dt')\n",
    "There are 25 regressors available in the model library of PyCaret. Please view the create_model() docstring for the list of all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create model\n",
    "number_of_models = 1\n",
    "\n",
    "\n",
    "def model_to_abv(model):\n",
    "    return model\n",
    "def train_and_plot_model(model, _auto_tune, plots = True):\n",
    "    print(\"Training {}\".format(model))\n",
    "    if _auto_tune:\n",
    "        print('Training with default hyper parameters')\n",
    "        trained_model = tune_model(model_to_abv(model))\n",
    "        print(\"Completed trained with default hyperparameters tuned {}\".format(trained_model))\n",
    "    else:\n",
    "        print(\"Hyperparameter tuning... {}\".format(_auto_tune))\n",
    "        trained_model = create_model(model_to_abv(model))\n",
    "        print(\"Completed training with hyperparameters tuned {}\".format(trained_model))\n",
    "    return trained_model \n",
    "trained_model = train_and_plot_model('gbr',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When a model is created using the create_model() function it uses the default hyperparameters. \n",
    "In order to tune hyperparameters, the tune_model() function is used. This function \n",
    "automatically tunes the hyperparameters of a model on a pre-defined search space and scores \n",
    "it using k-fold cross validation. The output prints a score grid that shows \n",
    "MAE, MSE, RMSE, R2, RMSLE and MAPE by fold.\n",
    "\n",
    "Note: tune_model() does not take a trained model object as an input. \n",
    "It instead requires a model name to be passed as an abbreviated string similar to \n",
    "how it is passed in create_model(). All other functions inpycaret.regression \n",
    "require a trained model object as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = True\n",
    "if plots == True:\n",
    "    print('Hyper parameters')\n",
    "    display(plot_model(trained_model, plot = 'parameter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# Residual Plot\n",
    "print('Residual plots')\n",
    "print(plot_model(trained_model))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importance plot\")\n",
    "plot_model(trained_model, plot = 'feature')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Error Plot\n",
    "print(\"Prediction Error plot\")\n",
    "plot_model(trained_model, plot = 'error')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning\n",
    "print(\"Learning Curve plot\")\n",
    "display(plot_model(trained_model, plot = 'learning'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Selection\n",
    "print(\"Recursive Feature Selection\")\n",
    "plot_model(trained_model, plot = 'rfe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooks Distance Plot\n",
    "print(\"Cooks Distance Plot\")\n",
    "plot_model(trained_model, plot = 'cooks')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Curve Plot\n",
    "print(\"Validation Curve Plot\")\n",
    "display(plot_model(trained_model, plot = 'vc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manifold \n",
    "print(\"manifold\")\n",
    "display(plot_model(trained_model, plot = 'manifold'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test / Hold-out Sample\n",
    "Before finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics. If you look at the information grid in Section 6 above, you will see that 30% (1621 samples) of the data has been separated out as a test/hold-out sample. All of the evaluation metrics we have seen above are cross-validated results based on training set (70%) only. Now, using our final trained model stored in the tuned_lightgbm variable we will predict the hold-out sample and evaluate the metrics to see if they are materially different than the CV results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize Model for Deployment\n",
    "Model finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The finalize_model() function fits the model onto the complete dataset including the test/hold-out sample (30% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = predict_model(trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 0.8519\n",
    "md(f\"The R2 on the test/hold-out set is 0.9728 { r2 } compared to 0.9753 achieved on tuned_lightgbm CV results (in section 9.2 above). This is not a significant difference. If there is a large variation between the test/hold-out and CV results, then this would normally indicate over-fitting but could also be due to several other factors and would require further investigation. In this case, we will move forward with finalizing the model and predicting on unseen data (the 10% that we had separated in the beginning and never exposed to PyCaret).(TIP : It's always good to look at the standard deviation of CV results when using create_model().)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Unseen Data\n",
    "The predict_model() function is also used to predict on the unseen dataset. The only difference from section 11 above is that this time we will pass the data_unseen parameter. data_unseen is the variable created at the beginning of the tutorial and contains 10% (600 samples) of the original dataset which was never exposed to PyCaret. (see section 5 for explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_predictions = predict_model(final_model, data=data_unseen)\n",
    "unseen_predictions.head()\n",
    "unseen_predictions_file = 'output/unseen_prediction.csv'\n",
    "unseen_predictions.to_csv(unseen_predictions_file)\n",
    "print('Exported {}'.format(unseen_predictions_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model / Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pickling model..\")\n",
    "save_model(final_model,'output/FinalModel_27May2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'Linear Regression':'lr',\n",
    "'Lasso Regression':'lasso',\n",
    "'Ridge Regression':'ridge',\n",
    "'Elastic Net':'en',\n",
    "'Least Angle Regression':'lar',\n",
    "'Lasso Least Angle Regression':'llar',\n",
    "'Orthogonal Matching Pursuit':'omp',\n",
    "'Bayesian Ridge':'br',\n",
    "'Automatic Relevance Determination':'ard',\n",
    "'Passive Aggressive Regressor':'par',\n",
    "'Random Sample Consensus':'ransac',\n",
    "'TheilSen Regressor':'tr',\n",
    "'Huber Regressor':'huber',\n",
    "'Kernel Ridge':'kr',\n",
    "'Support Vector Machine':'svm',\n",
    "'K Neighbors Regressor':'knn',\n",
    "'Decision Tree':'dt',\n",
    "'Random Forest':'rf',\n",
    "'Extra Trees Regressor':'et',\n",
    "'AdaBoost Regressor':'ada',\n",
    "'Gradient Boosting Regressor':'gbr',\n",
    "'Multi Level Perceptron':'mlp',\n",
    "'Extreme Gradient Boosting':'xgboost',\n",
    "'Light Gradient Boosting':'lightgbm',\n",
    "'CatBoost Regressor':'catboost'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
